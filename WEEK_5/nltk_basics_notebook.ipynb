{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbd4eb74",
   "metadata": {},
   "source": [
    "# NLTK Basics — A Beginner-Friendly Notebook\n",
    "\n",
    "**Goals:**\n",
    "- Learn how to set up NLTK and required data\n",
    "- Practice tokenization, stopword filtering, stemming, lemmatizing, POS tagging\n",
    "- Try chunking, chinking and basic Named Entity Recognition (NER)\n",
    "- See simple visualizations (frequency plots)\n",
    "\n",
    "**How to use this notebook:** run each cell in order. Cells that install packages are provided for convenience — if your environment already has the packages, you can skip those cells.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df68171d",
   "metadata": {},
   "source": [
    "## 1) Install & Setup\n",
    "\n",
    "Run this cell if you need to install packages. It uses `pip` — comment it out if you run in an environment where installation is not allowed.\n",
    "\n",
    "**Note:** this notebook was written for Python 3.9+ and NLTK 3.5 (you can use newer NLTK versions too).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "065c13e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Install step: run only if needed.\n"
     ]
    }
   ],
   "source": [
    "# Install (uncomment to run if you need it)\n",
    "# !python -m pip install --upgrade pip\n",
    "# !python -m pip install nltk==3.5 numpy matplotlib\n",
    "\n",
    "print('Install step: run only if needed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abfcdc92",
   "metadata": {},
   "source": [
    "## 2) Download NLTK data\n",
    "\n",
    "NLTK requires some datasets/models (tokenizers, taggers, corpora). Run the cell below once to download them to your environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ceb163cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading punkt\n",
      "Downloading stopwords\n",
      "Downloading averaged_perceptron_tagger\n",
      "Downloading wordnet\n",
      "Downloading omw-1.4\n",
      "Downloading maxent_ne_chunker\n",
      "Downloading words\n",
      "\n",
      "All required NLTK data attempted for download.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ROUCHI\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ROUCHI\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\ROUCHI\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ROUCHI\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\ROUCHI\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\ROUCHI\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\ROUCHI\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# These downloads are generally required for the examples in this notebook\n",
    "nltk_data = ['punkt', 'stopwords', 'averaged_perceptron_tagger', 'wordnet', 'omw-1.4',\n",
    "             'maxent_ne_chunker', 'words']\n",
    "for pkg in nltk_data:\n",
    "    print('Downloading', pkg)\n",
    "    nltk.download(pkg)\n",
    "\n",
    "print('\\nAll required NLTK data attempted for download.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de01c578",
   "metadata": {},
   "source": [
    "## 3) Common imports\n",
    "\n",
    "This cell imports the functions we will use repeatedly. Run it once near the top of the notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a2f63b80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports ready\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, SnowballStemmer, WordNetLemmatizer\n",
    "from nltk.probability import FreqDist\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "print('Imports ready')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227fc83f",
   "metadata": {},
   "source": [
    "## 4) Tokenization — sentences & words\n",
    "\n",
    "Tokenization splits text into sentences or words. This is almost always the first preprocessing step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3357c806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Sentence tokenization ---\n",
      "1 Muad'Dib learned rapidly because his first training was in how to learn.\n",
      "2 And the first lesson of all was the basic trust that he could learn.\n",
      "3 It's shocking to find how many people do not believe they can learn,\n",
      "and how many more believe learning to be difficult.\n",
      "\n",
      "--- Word tokenization ---\n",
      "[\"Muad'Dib\", 'learned', 'rapidly', 'because', 'his', 'first', 'training', 'was', 'in', 'how', 'to', 'learn', '.', 'And', 'the', 'first', 'lesson', 'of', 'all', 'was', 'the', 'basic', 'trust', 'that', 'he', 'could', 'learn', '.', 'It', \"'s\", 'shocking', 'to', 'find', 'how', 'many', 'people', 'do', 'not', 'believe', 'they', 'can', 'learn', ',', 'and', 'how', 'many', 'more', 'believe', 'learning', 'to', 'be', 'difficult', '.']\n",
      "\n",
      "Top words: [('how', 3), ('to', 3), ('learn', 3), ('first', 2), ('was', 2), ('and', 2), ('the', 2), ('many', 2)]\n"
     ]
    }
   ],
   "source": [
    "example_string = \"\"\"Muad'Dib learned rapidly because his first training was in how to learn.\n",
    "And the first lesson of all was the basic trust that he could learn.\n",
    "It's shocking to find how many people do not believe they can learn,\n",
    "and how many more believe learning to be difficult.\"\"\"\n",
    "\n",
    "print('--- Sentence tokenization ---')\n",
    "sentences = sent_tokenize(example_string)\n",
    "for i,s in enumerate(sentences,1):\n",
    "    print(i, s)\n",
    "\n",
    "print('\\n--- Word tokenization ---')\n",
    "words = word_tokenize(example_string)\n",
    "print(words)\n",
    "\n",
    "# Simple frequency on alphabetic tokens\n",
    "alpha_words = [w.lower() for w in words if w.isalpha()]\n",
    "print('\\nTop words:', Counter(alpha_words).most_common(8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd91b72",
   "metadata": {},
   "source": [
    "## 5) Stop words — filtering\n",
    "\n",
    "Stop words are common words that you may want to remove for many tasks (but *not* always — negation words like 'not' may be important!).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "517a96f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tokens: ['Sir', ',', 'I', 'protest', '.', 'I', 'am', 'not', 'a', 'merry', 'man', '!']\n",
      "\n",
      "Filtered (default NLTK stopwords removed): ['Sir', ',', 'protest', '.', 'merry', 'man', '!']\n",
      "\n",
      "Filtered with custom stoplist (kept \"not\", \"i\"): ['Sir', ',', 'I', 'protest', '.', 'I', 'not', 'merry', 'man', '!']\n"
     ]
    }
   ],
   "source": [
    "worf_quote = \"Sir, I protest. I am not a merry man!\"\n",
    "words_in_quote = word_tokenize(worf_quote)\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "print('Original tokens:', words_in_quote)\n",
    "filtered = [w for w in words_in_quote if w.casefold() not in stop_words]\n",
    "print('\\nFiltered (default NLTK stopwords removed):', filtered)\n",
    "\n",
    "# Example: keep 'not' and 'i' because they may be important\n",
    "custom_stop = stop_words - {'not', 'i'}\n",
    "filtered_keep_not = [w for w in words_in_quote if w.casefold() not in custom_stop]\n",
    "print('\\nFiltered with custom stoplist (kept \"not\", \"i\"):', filtered_keep_not)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b42fbdf",
   "metadata": {},
   "source": [
    "## 6) Stemming — crude root forms\n",
    "\n",
    "Stemmers reduce words to a root form — results may be non-words. Try Porter vs Snowball.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "33648f62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['The', 'crew', 'of', 'the', 'USS', 'Discovery', 'discovered', 'many', 'discoveries', '.', 'Discovering', 'is', 'what', 'explorers', 'do', '.']\n",
      "\n",
      "Porter stems:\n",
      "['the', 'crew', 'of', 'the', 'uss', 'discoveri', 'discov', 'mani', 'discoveri', '.', 'discov', 'is', 'what', 'explor', 'do', '.']\n",
      "\n",
      "Snowball stems:\n",
      "['the', 'crew', 'of', 'the', 'uss', 'discoveri', 'discov', 'mani', 'discoveri', '.', 'discov', 'is', 'what', 'explor', 'do', '.']\n"
     ]
    }
   ],
   "source": [
    "porter = PorterStemmer()\n",
    "snow = SnowballStemmer('english')\n",
    "string_for_stemming = \"The crew of the USS Discovery discovered many discoveries. Discovering is what explorers do.\"\n",
    "tokens = word_tokenize(string_for_stemming)\n",
    "print('Tokens:', tokens)\n",
    "print('\\nPorter stems:')\n",
    "print([porter.stem(t) for t in tokens])\n",
    "print('\\nSnowball stems:')\n",
    "print([snow.stem(t) for t in tokens])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34bc26cd",
   "metadata": {},
   "source": [
    "## 7) Part-of-Speech (POS) tagging\n",
    "\n",
    "POS tagging labels each token with a part-of-speech tag (noun, verb, adjective ...). Useful for downstream tasks like lemmatization, chunking.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "abac3554",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens and POS tags:\n",
      "[('If', 'IN'), ('you', 'PRP'), ('wish', 'VBP'), ('to', 'TO'), ('make', 'VB'), ('an', 'DT'), ('apple', 'NN'), ('pie', 'NN'), ('from', 'IN'), ('scratch', 'NN'), (',', ','), ('you', 'PRP'), ('must', 'MD'), ('first', 'VB'), ('invent', 'VB'), ('the', 'DT'), ('universe', 'NN'), ('.', '.')]\n",
      "\n",
      "Sample tag meanings:\n",
      "NN → noun, VB → verb, JJ → adjective, RB → adverb, PRP → pronoun\n"
     ]
    }
   ],
   "source": [
    "sagan_quote = \"If you wish to make an apple pie from scratch, you must first invent the universe.\"\n",
    "tokens = word_tokenize(sagan_quote)\n",
    "pos_tags = nltk.pos_tag(tokens)\n",
    "print('Tokens and POS tags:')\n",
    "print(pos_tags)\n",
    "\n",
    "# Useful: inspect a small subset of the tagset\n",
    "print('\\nSample tag meanings:')\n",
    "print('NN → noun, VB → verb, JJ → adjective, RB → adverb, PRP → pronoun')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7d6efc",
   "metadata": {},
   "source": [
    "## 8) Lemmatization — dictionary root forms (better than stemming when done with POS)\n",
    "\n",
    "Lemmatizers produce real words. Supplying the POS to the lemmatizer improves results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c5bfbddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word | POS | Lemma\n",
      "The | DT | The\n",
      "striped | JJ | striped\n",
      "bats | NNS | bat\n",
      "are | VBP | be\n",
      "hanging | VBG | hang\n",
      "on | IN | on\n",
      "their | PRP$ | their\n",
      "feet | NNS | foot\n",
      "for | IN | for\n",
      "best | JJS | best\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "text = \"The striped bats are hanging on their feet for best\"\n",
    "tokens = word_tokenize(text)\n",
    "tags = nltk.pos_tag(tokens)\n",
    "lemmas = [lemmatizer.lemmatize(w, get_wordnet_pos(t)) for w,t in tags]\n",
    "print('Word | POS | Lemma')\n",
    "for w,t,l in zip(tokens, tags, lemmas):\n",
    "    print(w, '|', t[1], '|', l)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0385f5",
   "metadata": {},
   "source": [
    "## 9) Chunking and Chinking\n",
    "\n",
    "Chunking groups tokens into phrases (e.g., noun phrases). Chinking removes patterns from chunks.\n",
    "\n",
    "Note: GUI tree drawing (tree.draw()) may not work in headless environments — we'll show textual outputs instead.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a0e32d69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens with POS:\n",
      "[('It', 'PRP'), (\"'s\", 'VBZ'), ('a', 'DT'), ('dangerous', 'JJ'), ('business', 'NN'), (',', ','), ('Frodo', 'NNP'), (',', ','), ('going', 'VBG'), ('out', 'RP'), ('your', 'PRP$'), ('door', 'NN'), ('.', '.')]\n",
      "\n",
      "Chunk parse tree (text):\n",
      "(S\n",
      "  It/PRP\n",
      "  's/VBZ\n",
      "  (NP a/DT dangerous/JJ business/NN)\n",
      "  ,/,\n",
      "  Frodo/NNP\n",
      "  ,/,\n",
      "  going/VBG\n",
      "  out/RP\n",
      "  your/PRP$\n",
      "  (NP door/NN)\n",
      "  ./.)\n",
      "\n",
      "Extracted noun phrases:\n",
      "a dangerous business\n",
      "door\n",
      "\n",
      "Chinked tree (text):\n",
      "(S\n",
      "  (Chunk It/PRP 's/VBZ a/DT)\n",
      "  dangerous/JJ\n",
      "  (Chunk\n",
      "    business/NN\n",
      "    ,/,\n",
      "    Frodo/NNP\n",
      "    ,/,\n",
      "    going/VBG\n",
      "    out/RP\n",
      "    your/PRP$\n",
      "    door/NN\n",
      "    ./.))\n"
     ]
    }
   ],
   "source": [
    "lotr_quote = \"It's a dangerous business, Frodo, going out your door.\"\n",
    "tokens = word_tokenize(lotr_quote)\n",
    "pos = nltk.pos_tag(tokens)\n",
    "print('Tokens with POS:')\n",
    "print(pos)\n",
    "\n",
    "# Define a simple noun phrase grammar: optional determiner, any number of adjectives, then a noun\n",
    "grammar = 'NP: {<DT>?<JJ>*<NN>}'\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "tree = cp.parse(pos)\n",
    "print('\\nChunk parse tree (text):')\n",
    "print(tree)\n",
    "\n",
    "print('\\nExtracted noun phrases:')\n",
    "for subtree in tree.subtrees(filter=lambda t: t.label()=='NP'):\n",
    "    print(' '.join(word for word,pos in subtree.leaves()))\n",
    "\n",
    "# Chinking example: exclude adjectives (JJ) from chunks\n",
    "grammar2 = r\"\"\"Chunk: {<.*>+}\n",
    "       }<JJ>{\n",
    "\"\"\"\n",
    "cp2 = nltk.RegexpParser(grammar2)\n",
    "tree2 = cp2.parse(pos)\n",
    "print('\\nChinked tree (text):')\n",
    "print(tree2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6397cec",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercises (try these)\n",
    "1. Create a function that accepts a text and returns the top 5 lemmatized content words (no stopwords).\n",
    "2. Compare stemming vs lemmatization on a small news paragraph — what differences do you see?\n",
    "3. Build a small named-entity extractor that also returns the entity label (PERSON, GPE, ORGANIZATION). Try it on a news blurb.\n",
    "4. Try chunking to extract verb phrases instead of noun phrases.\n",
    "\n",
    "## Further reading\n",
    "- NLTK Book: *Natural Language Processing with Python* (Bird, Klein, Loper)\n",
    "- NLTK documentation: https://www.nltk.org\n",
    "\n",
    "Good luck — experiment and have fun!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
